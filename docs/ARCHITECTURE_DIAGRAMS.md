# StateSet Agents - Architecture Diagrams

Visual reference for understanding the framework's architecture.

## System Overview

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                         STATESET AGENTS FRAMEWORK                            │
│                    Production-Ready RL for Conversational AI                 │
└──────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│  USER APPLICATION                                                            │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐        │
│  │  Python     │  │   CLI       │  │  REST API   │  │  WebSocket  │        │
│  │  Library    │  │  Interface  │  │  Client     │  │  Client     │        │
│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘        │
└─────────┼────────────────┼────────────────┼────────────────┼────────────────┘
          │                │                │                │
          ▼                ▼                ▼                ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  API LAYER                                                                   │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │  FastAPI Server (api/main.py)                                        │   │
│  │  ┌────────────┐ ┌────────────┐ ┌────────────┐ ┌────────────┐        │   │
│  │  │ /training  │ │ /agents    │ │ /chat      │ │ /metrics   │        │   │
│  │  │ Start/Stop │ │ CRUD       │ │ Inference  │ │ Prometheus │        │   │
│  │  └────────────┘ └────────────┘ └────────────┘ └────────────┘        │   │
│  │  ┌────────────┐ ┌────────────┐ ┌────────────┐ ┌────────────┐        │   │
│  │  │ Auth       │ │ Rate Limit │ │ Circuit    │ │ Health     │        │   │
│  │  │ JWT/APIKey │ │ Throttle   │ │ Breaker    │ │ Checks     │        │   │
│  │  └────────────┘ └────────────┘ └────────────┘ └────────────┘        │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────────┘
          │
          ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  TRAINING LAYER                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Algorithm Selection                                                 │    │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐  │    │
│  │  │  GSPO    │ │  VAPO    │ │  DAPO    │ │  GRPO    │ │  PPO     │  │    │
│  │  │ Stable   │ │ Reasoning│ │ Long CoT │ │ Baseline │ │ Classic  │  │    │
│  │  │ Default  │ │ SOTA     │ │ Dynamic  │ │ Fast     │ │ Reliable │  │    │
│  │  └──────────┘ └──────────┘ └──────────┘ └──────────┘ └──────────┘  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Training Infrastructure                                             │    │
│  │  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐                 │    │
│  │  │ BaseTrainer  │ │ Distributed  │ │ HPO          │                 │    │
│  │  │ Shared logic │ │ Multi-GPU    │ │ Optuna/Ray   │                 │    │
│  │  └──────────────┘ └──────────────┘ └──────────────┘                 │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────────────────────┘
          │
          ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  CORE LAYER                                                                  │
│  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐       │
│  │    Agent     │ │ Environment  │ │   Reward     │ │  Trajectory  │       │
│  │              │ │              │ │              │ │              │       │
│  │ MultiTurn    │ │ Conversation │ │ Composite    │ │ MultiTurn    │       │
│  │ ToolAgent    │ │ Task         │ │ LLM-Judge    │ │ Group        │       │
│  │ Stub         │ │ Simulated    │ │ Bayesian     │ │ Batch        │       │
│  └──────────────┘ └──────────────┘ └──────────────┘ └──────────────┘       │
│  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐                        │
│  │ Value Fn     │ │ Comp Engine  │ │ Config       │                        │
│  │ GAE          │ │ Parallel Gen │ │ Profiles     │                        │
│  │ Critic       │ │ Buffering    │ │ Presets      │                        │
│  └──────────────┘ └──────────────┘ └──────────────┘                        │
└─────────────────────────────────────────────────────────────────────────────┘
          │
          ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│  INFRASTRUCTURE LAYER                                                        │
│  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐       │
│  │ HuggingFace  │ │    vLLM      │ │   PyTorch    │ │     TRL      │       │
│  │ Transformers │ │ Acceleration │ │   Backend    │ │ Integration  │       │
│  └──────────────┘ └──────────────┘ └──────────────┘ └──────────────┘       │
│  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐       │
│  │    PEFT      │ │  DeepSpeed   │ │  Accelerate  │ │   W&B        │       │
│  │    LoRA      │ │  ZeRO        │ │  DDP         │ │  Logging     │       │
│  └──────────────┘ └──────────────┘ └──────────────┘ └──────────────┘       │
└─────────────────────────────────────────────────────────────────────────────┘
```

## Training Pipeline Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           GRPO/GSPO TRAINING LOOP                           │
└─────────────────────────────────────────────────────────────────────────────┘

  ┌─────────────┐
  │   START     │
  └──────┬──────┘
         │
         ▼
  ┌─────────────────────────────────────────────────────────────────┐
  │  1. INITIALIZATION                                               │
  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐             │
  │  │ Load Model  │  │ Setup LoRA  │  │ Init Optim  │             │
  │  │ & Tokenizer │  │ Adapters    │  │ & Scheduler │             │
  │  └─────────────┘  └─────────────┘  └─────────────┘             │
  └──────────────────────────┬──────────────────────────────────────┘
                             │
         ┌───────────────────┴───────────────────┐
         │          TRAINING LOOP                │
         │         (num_episodes)                │
         └───────────────────┬───────────────────┘
                             │
                             ▼
  ┌─────────────────────────────────────────────────────────────────┐
  │  2. TRAJECTORY GENERATION                                        │
  │                                                                  │
  │    ┌─────────┐      ┌─────────────────────────┐                 │
  │    │ Prompt  │─────▶│  Generate G Responses   │                 │
  │    │ Pool    │      │  (vLLM or HuggingFace)  │                 │
  │    └─────────┘      └───────────┬─────────────┘                 │
  │                                 │                                │
  │    ┌────────────────────────────┼────────────────────────────┐  │
  │    │         Response Group (G=4 shown)                      │  │
  │    │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐       │  │
  │    │  │ Resp 1  │ │ Resp 2  │ │ Resp 3  │ │ Resp 4  │       │  │
  │    │  │ logP=-2 │ │ logP=-3 │ │ logP=-2 │ │ logP=-4 │       │  │
  │    │  └─────────┘ └─────────┘ └─────────┘ └─────────┘       │  │
  │    └─────────────────────────────────────────────────────────┘  │
  └──────────────────────────┬──────────────────────────────────────┘
                             │
                             ▼
  ┌─────────────────────────────────────────────────────────────────┐
  │  3. REWARD COMPUTATION                                           │
  │                                                                  │
  │    ┌───────────────────────────────────────────────────────┐    │
  │    │              Composite Reward Function                 │    │
  │    │  ┌────────────┐ ┌────────────┐ ┌────────────┐        │    │
  │    │  │ Helpfulness│ │   Safety   │ │ Engagement │        │    │
  │    │  │   w=0.4    │ │   w=0.3    │ │   w=0.3    │        │    │
  │    │  └─────┬──────┘ └─────┬──────┘ └─────┬──────┘        │    │
  │    │        │              │              │                │    │
  │    │        └──────────────┼──────────────┘                │    │
  │    │                       ▼                               │    │
  │    │              ┌────────────────┐                       │    │
  │    │              │ Final Rewards  │                       │    │
  │    │              │ R=[0.8, 0.3,   │                       │    │
  │    │              │    0.9, 0.2]   │                       │    │
  │    │              └────────────────┘                       │    │
  │    └───────────────────────────────────────────────────────┘    │
  └──────────────────────────┬──────────────────────────────────────┘
                             │
                             ▼
  ┌─────────────────────────────────────────────────────────────────┐
  │  4. ADVANTAGE COMPUTATION                                        │
  │                                                                  │
  │    Rewards:    [0.8,  0.3,  0.9,  0.2]                          │
  │    Baseline:   mean = 0.55                                       │
  │                  ─────────────────────                          │
  │    Advantages: [0.25, -0.25, 0.35, -0.35]                       │
  │                                                                  │
  │    ┌──────────────────────────────────────────────────────┐     │
  │    │  GSPO: Sequence-level clipping [1-ε, 1+ε]            │     │
  │    │  VAPO: Length-adaptive GAE (λ = 1 - 1/(αL))          │     │
  │    │  DAPO: Clip-higher [1-0.2, 1+0.28]                   │     │
  │    └──────────────────────────────────────────────────────┘     │
  └──────────────────────────┬──────────────────────────────────────┘
                             │
                             ▼
  ┌─────────────────────────────────────────────────────────────────┐
  │  5. POLICY UPDATE                                                │
  │                                                                  │
  │    ┌─────────────────────────────────────────────────────────┐  │
  │    │                                                         │  │
  │    │   L_policy = -Σ advantage_i × log π(response_i|prompt) │  │
  │    │                                                         │  │
  │    │   L_kl = β × KL(π || π_ref)    [optional]              │  │
  │    │                                                         │  │
  │    │   L_total = L_policy + L_kl                            │  │
  │    │                                                         │  │
  │    └─────────────────────────────────────────────────────────┘  │
  │                                                                  │
  │    ┌──────────┐   ┌──────────┐   ┌──────────┐   ┌──────────┐   │
  │    │ Backward │──▶│ Clip Grad│──▶│ Optimizer│──▶│ Scheduler│   │
  │    │   Pass   │   │ max_norm │   │   Step   │   │   Step   │   │
  │    └──────────┘   └──────────┘   └──────────┘   └──────────┘   │
  └──────────────────────────┬──────────────────────────────────────┘
                             │
                             ▼
  ┌─────────────────────────────────────────────────────────────────┐
  │  6. LOGGING & CHECKPOINTING                                      │
  │                                                                  │
  │    ┌──────────┐   ┌──────────┐   ┌──────────┐                  │
  │    │   W&B    │   │ Console  │   │ Checkpoint│                  │
  │    │  Metrics │   │   Log    │   │   Save    │                  │
  │    └──────────┘   └──────────┘   └──────────┘                  │
  └──────────────────────────┬──────────────────────────────────────┘
                             │
                             ▼
                    ┌────────────────┐
                    │  Next Episode  │──────────┐
                    └────────────────┘          │
                             │                  │
                    ┌────────┴────────┐         │
                    │ episodes < max? │─── Yes ─┘
                    └────────┬────────┘
                             │ No
                             ▼
                    ┌─────────────┐
                    │    END      │
                    └─────────────┘
```

## Algorithm Comparison

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                        RL ALGORITHM COMPARISON                               │
└──────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│  GSPO (Group Sequence Policy Optimization)                                   │
│  ═══════════════════════════════════════════                                │
│                                                                              │
│  Key Innovation: Sequence-level importance ratios                            │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  ratio = π(y|x) / π_old(y|x)    [SEQUENCE level, not token]         │    │
│  │  loss = -min(ratio × A, clip(ratio, 1-ε, 1+ε) × A)                  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Strengths: ✓ Prevents model collapse  ✓ Stable for long sequences         │
│             ✓ Native MoE support       ✓ Powers Qwen3                       │
│  Best for:  General conversational AI, production deployments                │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│  VAPO (Value-Augmented Policy Optimization)                                  │
│  ═══════════════════════════════════════════                                │
│                                                                              │
│  Key Innovations (7 total):                                                  │
│  ┌──────────────────────────────────────────────────────────────────┐       │
│  │  1. Value warmup (50 steps)                                      │       │
│  │  2. Decoupled GAE (λ_critic=1.0, λ_policy=adaptive)             │       │
│  │  3. Length-adaptive λ: λ = 1 - 1/(α × length)                   │       │
│  │  4. Clip-higher: [1-0.2, 1+0.28] asymmetric                     │       │
│  │  5. Token-level loss normalization                               │       │
│  │  6. Positive example LM loss                                     │       │
│  │  7. Group sampling (more samples/prompt)                         │       │
│  └──────────────────────────────────────────────────────────────────┘       │
│                                                                              │
│  Results: 60.4 on AIME 2024 (SOTA for math reasoning)                       │
│  Best for: Mathematical reasoning, long chain-of-thought                     │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│  DAPO (Decoupled Clip and Dynamic Sampling PO)                               │
│  ═════════════════════════════════════════════                               │
│                                                                              │
│  Key Innovations (4 total):                                                  │
│  ┌──────────────────────────────────────────────────────────────────┐       │
│  │  1. Clip-Higher: ε_low=0.2, ε_high=0.28                         │       │
│  │     └── Allows more exploration on positive advantages           │       │
│  │                                                                  │       │
│  │  2. Dynamic Sampling: Filter 0%/100% accuracy prompts            │       │
│  │     └── Ensures all gradients are meaningful                     │       │
│  │                                                                  │       │
│  │  3. Token-Level Loss: Normalize by token count                   │       │
│  │     └── Prevents length bias                                     │       │
│  │                                                                  │       │
│  │  4. Overlong Reward Shaping:                                     │       │
│  │     └── Soft penalty: 0 → -1 as length → max                     │       │
│  └──────────────────────────────────────────────────────────────────┘       │
│                                                                              │
│  Results: 50 points on AIME 2024                                            │
│  Best for: Long-form generation, coding tasks                                │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│  GRPO (Group Relative Policy Optimization)                                   │
│  ═════════════════════════════════════════                                   │
│                                                                              │
│  Classic baseline algorithm:                                                 │
│  ┌──────────────────────────────────────────────────────────────────┐       │
│  │  advantage_i = reward_i - mean(rewards_in_group)                 │       │
│  │  loss = -Σ advantage_i × log π(response_i|prompt)               │       │
│  └──────────────────────────────────────────────────────────────────┘       │
│                                                                              │
│  Strengths: ✓ Simple to implement  ✓ Fast iteration                         │
│  Weaknesses: Can be unstable for long sequences                              │
│  Best for: Quick experiments, baseline comparisons                           │
└─────────────────────────────────────────────────────────────────────────────┘
```

## Data Flow Architecture

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                             DATA FLOW                                        │
└──────────────────────────────────────────────────────────────────────────────┘

┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   Prompts   │     │   Agent     │     │  Responses  │     │  Rewards    │
│   (Input)   │────▶│  (Model)    │────▶│  (Output)   │────▶│  (Scores)   │
└─────────────┘     └─────────────┘     └─────────────┘     └─────────────┘
      │                   │                   │                   │
      │                   │                   │                   │
      ▼                   ▼                   ▼                   ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                              │
│   prompts: ["How do I..."]  →  responses: ["You can..."]  →  rewards: [0.8] │
│                                                                              │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                     MultiTurnTrajectory                              │   │
│   │                                                                      │   │
│   │   trajectory_id: "traj_001"                                         │   │
│   │   turns: [                                                          │   │
│   │     ConversationTurn(role="user", content="How do I...")           │   │
│   │     ConversationTurn(role="assistant", content="You can...")       │   │
│   │   ]                                                                 │   │
│   │   rewards: [0.0, 0.8]  # per-turn                                  │   │
│   │   total_reward: 0.8                                                 │   │
│   │   metadata: {...}                                                   │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │                       TrajectoryGroup                                │   │
│   │                                                                      │   │
│   │   trajectories: [traj_1, traj_2, traj_3, traj_4]                   │   │
│   │   rewards: [0.8, 0.3, 0.9, 0.2]                                    │   │
│   │   advantages: [0.25, -0.25, 0.35, -0.35]                           │   │
│   │                                                                      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

## Deployment Architecture

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                        KUBERNETES DEPLOYMENT                                 │
└──────────────────────────────────────────────────────────────────────────────┘

                              ┌──────────────┐
                              │   Ingress    │
                              │   (nginx)    │
                              └──────┬───────┘
                                     │
                    ┌────────────────┼────────────────┐
                    │                │                │
                    ▼                ▼                ▼
            ┌──────────────┐ ┌──────────────┐ ┌──────────────┐
            │   API Pod    │ │   API Pod    │ │   API Pod    │
            │   (replica)  │ │   (replica)  │ │   (replica)  │
            └──────┬───────┘ └──────┬───────┘ └──────┬───────┘
                   │                │                │
                   └────────────────┼────────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    │                               │
                    ▼                               ▼
            ┌──────────────┐               ┌──────────────┐
            │   Redis      │               │  PostgreSQL  │
            │   Cache      │               │   Storage    │
            └──────────────┘               └──────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│  TRAINING CLUSTER                                                            │
│                                                                              │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐        │
│  │ GPU Node 0  │  │ GPU Node 1  │  │ GPU Node 2  │  │ GPU Node 3  │        │
│  │   A100      │  │   A100      │  │   A100      │  │   A100      │        │
│  │   80GB      │  │   80GB      │  │   80GB      │  │   80GB      │        │
│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘        │
│         │                │                │                │                │
│         └────────────────┴────────────────┴────────────────┘                │
│                                   │                                         │
│                          ┌────────┴────────┐                               │
│                          │ DeepSpeed ZeRO  │                               │
│                          │ Distributed     │                               │
│                          └─────────────────┘                               │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│  MONITORING STACK                                                            │
│                                                                              │
│  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐   ┌─────────────┐     │
│  │ Prometheus  │──▶│  Grafana    │   │    W&B      │   │   Alerts    │     │
│  │  Metrics    │   │  Dashboards │   │  Experiment │   │   Slack     │     │
│  └─────────────┘   └─────────────┘   └─────────────┘   └─────────────┘     │
└─────────────────────────────────────────────────────────────────────────────┘
```

## Reward System Architecture

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                         REWARD SYSTEM                                        │
└──────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│  CompositeReward                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │    ┌────────────────┐                                               │    │
│  │    │ HelpfulnessReward │─────┐                                      │    │
│  │    │    weight=0.35    │     │                                      │    │
│  │    └────────────────┘       │                                      │    │
│  │                              │                                      │    │
│  │    ┌────────────────┐       │    ┌─────────────────┐              │    │
│  │    │  SafetyReward    │─────┼───▶│  Weighted Sum   │──▶ score     │    │
│  │    │    weight=0.25    │     │    │  Σ(w_i × s_i)  │              │    │
│  │    └────────────────┘       │    └─────────────────┘              │    │
│  │                              │                                      │    │
│  │    ┌────────────────┐       │                                      │    │
│  │    │ EngagementReward │─────┘                                      │    │
│  │    │    weight=0.40    │                                            │    │
│  │    └────────────────┘                                              │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│  Reward Types                                                                │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Rule-Based                    │  Neural                            │    │
│  │  ┌──────────────┐             │  ┌──────────────┐                  │    │
│  │  │ Keyword      │             │  │ Learned      │                  │    │
│  │  │ Detection    │             │  │ Reward Model │                  │    │
│  │  └──────────────┘             │  └──────────────┘                  │    │
│  │  ┌──────────────┐             │  ┌──────────────┐                  │    │
│  │  │ Regex        │             │  │ LLM-as-Judge │                  │    │
│  │  │ Patterns     │             │  │ (RULER)      │                  │    │
│  │  └──────────────┘             │  └──────────────┘                  │    │
│  │  ┌──────────────┐             │  ┌──────────────┐                  │    │
│  │  │ Length       │             │  │ Bayesian     │                  │    │
│  │  │ Constraints  │             │  │ Reward       │                  │    │
│  │  └──────────────┘             │  └──────────────┘                  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────────────────────┘
```

## Extension Points

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                        EXTENSION POINTS                                      │
└──────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                              │
│  ┌──────────────┐     ┌──────────────┐     ┌──────────────┐                │
│  │ Custom Agent │     │ Custom Reward│     │ Custom Env   │                │
│  └──────┬───────┘     └──────┬───────┘     └──────┬───────┘                │
│         │                    │                    │                         │
│         ▼                    ▼                    ▼                         │
│  ┌──────────────┐     ┌──────────────┐     ┌──────────────┐                │
│  │ Inherit      │     │ Implement    │     │ Inherit      │                │
│  │ MultiTurnAgent│    │ RewardFunction│    │ Environment  │                │
│  └──────────────┘     └──────────────┘     └──────────────┘                │
│                                                                              │
│  Example:                                                                    │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  class MyAgent(MultiTurnAgent):                                      │   │
│  │      async def process_turn(self, history, user_input, context):    │   │
│  │          # Custom pre-processing                                     │   │
│  │          enhanced_context = self.add_domain_knowledge(context)       │   │
│  │          return await super().process_turn(                          │   │
│  │              history, user_input, enhanced_context                   │   │
│  │          )                                                           │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  class BusinessMetricReward(RewardFunction):                         │   │
│  │      async def compute_reward(self, turns, context=None):           │   │
│  │          conversion_score = self.check_conversion(turns)             │   │
│  │          satisfaction = self.measure_satisfaction(turns)             │   │
│  │          return RewardResult(                                        │   │
│  │              score=0.6*conversion_score + 0.4*satisfaction,          │   │
│  │              breakdown={"conversion": conversion_score, ...}         │   │
│  │          )                                                           │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```
