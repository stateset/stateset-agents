Metadata-Version: 2.1
Name: grpo-agent-framework
Version: 0.1.0
Summary: A comprehensive framework for training multi-turn AI agents using Group Relative Policy Optimization (GRPO)
Home-page: https://github.com/yourusername/grpo-agent-framework
Author: GRPO Framework Team
Author-email: team@grpo-framework.ai
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.0.0
Requires-Dist: transformers>=4.30.0
Requires-Dist: datasets>=2.0.0
Requires-Dist: numpy>=1.21.0
Requires-Dist: peft>=0.4.0
Requires-Dist: trl>=0.7.0
Requires-Dist: accelerate>=0.20.0
Requires-Dist: wandb>=0.15.0
Requires-Dist: tqdm>=4.65.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: typer>=0.9.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: black>=23.0.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Requires-Dist: sphinx>=6.0.0; extra == "dev"
Requires-Dist: sphinx-rtd-theme>=1.2.0; extra == "dev"
Provides-Extra: examples
Requires-Dist: openai>=1.0.0; extra == "examples"
Requires-Dist: anthropic>=0.5.0; extra == "examples"
Requires-Dist: langchain>=0.1.0; extra == "examples"

# GRPO Agent Framework

A comprehensive framework for training multi-turn AI agents using Group Relative Policy Optimization (GRPO).

## Overview

GRPO Agent Framework provides a flexible and robust infrastructure for training conversational AI agents that can handle multi-turn interactions. It implements state-of-the-art reinforcement learning techniques with a focus on stability, performance, and ease of use.

### Key Features

- ðŸš€ **Multi-turn Conversation Support**: Native support for training agents on extended dialogues
- ðŸŽ¯ **Flexible Reward Modeling**: Composable reward functions for complex objectives
- ðŸ”§ **Best Practices Built-in**: Automatic configuration optimization and stability monitoring
- ðŸ“Š **Comprehensive Diagnostics**: Real-time training health monitoring and adjustments
- ðŸ”Œ **Extensible Architecture**: Easy to add custom agents, environments, and rewards
- ðŸ¤– **Pre-built Components**: Ready-to-use agents for common tasks

## Installation

```bash
pip install grpo-agent-framework
```

For development:
```bash
git clone https://github.com/yourusername/grpo-agent-framework
cd grpo-agent-framework
pip install -e ".[dev]"
```

## Quick Start

### Training a Simple Agent

```python
from grpo_agent_framework import Agent, Environment, train

# Define your agent
agent = Agent.from_pretrained("gpt2")

# Create environment
env = Environment.from_task("conversation")

# Train
trainer = train(
    agent=agent,
    environment=env,
    num_episodes=1000,
    profile="balanced"  # or "conservative", "aggressive"
)
```

### Creating a Custom Multi-turn Agent

```python
from grpo_agent_framework import MultiTurnAgent, ConversationEnvironment
from grpo_agent_framework.rewards import CompositeReward, HelpfulnessReward, SafetyReward

# Define custom agent
class CustomerServiceAgent(MultiTurnAgent):
    def __init__(self, model_name="gpt2"):
        super().__init__(model_name)
        self.system_prompt = "You are a helpful customer service agent."
    
    def process_turn(self, history, user_input):
        # Custom turn processing logic
        return self.generate_response(history, user_input)

# Create environment with custom rewards
env = ConversationEnvironment(
    scenarios=load_scenarios("customer_service"),
    max_turns=10,
    reward_fn=CompositeReward([
        HelpfulnessReward(weight=0.5),
        SafetyReward(weight=0.3),
        ConcisenessReward(weight=0.2)
    ])
)

# Train with automatic configuration
from grpo_agent_framework.training import AutoTrainer

trainer = AutoTrainer()
trained_agent = trainer.train(
    agent=CustomerServiceAgent("gpt2"),
    environment=env,
    auto_adjust=True  # Automatically adjust hyperparameters
)
```

## Architecture

### Core Components

1. **Agents**: Base classes for implementing RL agents
   - `Agent`: Single-turn agent base class
   - `MultiTurnAgent`: Multi-turn conversation agent
   - `ToolAgent`: Agent with tool-use capabilities

2. **Environments**: Interaction environments for agents
   - `Environment`: Base environment class
   - `ConversationEnvironment`: Multi-turn conversation environment
   - `TaskEnvironment`: Task-oriented environment

3. **Rewards**: Flexible reward modeling system
   - `RewardFunction`: Base reward class
   - `CompositeReward`: Combine multiple rewards
   - Pre-built rewards: helpfulness, safety, accuracy, etc.

4. **Training**: GRPO training infrastructure
   - `GRPOTrainer`: Core training logic
   - `AutoTrainer`: Automatic hyperparameter optimization
   - `DiagnosticsMonitor`: Training health monitoring

## Advanced Features

### Automatic Configuration Optimization

```python
from grpo_agent_framework.training import ConfigOptimizer

optimizer = ConfigOptimizer()
config = optimizer.optimize_for_task(
    task_characteristics=analyzer.analyze(your_task),
    profile="balanced"
)
```

### Real-time Diagnostics

```python
from grpo_agent_framework.diagnostics import TrainingMonitor

monitor = TrainingMonitor()
trainer.add_callback(monitor)

# Get real-time health status
health = monitor.get_health_status()
print(f"Training health: {health.status}")
```

### Multi-GPU Training

```python
from grpo_agent_framework.training import DistributedTrainer

trainer = DistributedTrainer(
    num_gpus=4,
    strategy="ddp"  # or "deepspeed"
)
```

## Examples

See the `examples/` directory for:
- Customer service agent
- Code assistant agent
- Educational tutor agent
- Multi-modal conversation agent

## Documentation

Full documentation available at: https://grpo-framework.readthedocs.io

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md).

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Citation

If you use this framework in your research, please cite:

```bibtex
@software{grpo_agent_framework,
  title = {GRPO Agent Framework: A Comprehensive Framework for Training Multi-turn AI Agents},
  year = {2024},
  url = {https://github.com/yourusername/grpo-agent-framework}
}
```
