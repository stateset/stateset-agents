{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateset Agents: RL Capabilities Demo\n",
    "\n",
    "This Jupyter Notebook demonstrates the core capabilities of the `stateset-agents` framework by training a small conversational AI agent.\n",
    "\n",
    "We will cover:\n",
    "1.  **Multi-turn Agent Configuration**: Setting up an LLM-based conversational agent.\n",
    "2.  **Group Relative Policy Optimization (GRPO)**: Utilizing the framework's flagship RL algorithm.\n",
    "3.  **Rust Acceleration**: Implicitly leveraging the optimized `stateset-rl-core` for critical computations.\n",
    "4.  **Custom Reward Functions**: Defining a specific objective (politeness and conciseness) for the agent.\n",
    "\n",
    "The goal is to train a small model (Qwen/Qwen1.5-0.5B-Chat) to be:\n",
    "-   **Polite**: Always start responses with phrases like \"I appreciate\" or \"Thank you.\"\n",
    "-   **Concise**: Keep responses under 20 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, ensure you have the necessary dependencies installed. If running in a new environment, execute the following command (uncomment if needed):\n",
    "\n",
    "```bash\n",
    "# pip install -e "."  # Install stateset-agents in editable mode\n",
    "# pip install torch transformers numpy # Minimal required for this demo\n",
    "# pip install tiktoken accelerate transformers_stream_generator einops # Additional dependencies for Qwen\n",
    "```\n",
    "Also, ensure your `PYTHONPATH` includes the project root so `stateset_agents` can be imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# 1. Imports from the framework\n",
    "from stateset_agents.core.agent import MultiTurnAgent, AgentConfig\n",
    "from stateset_agents.core.environment import ConversationEnvironment\n",
    "from stateset_agents.training.multi_turn_trainer import MultiTurnGRPOTrainer\n",
    "from stateset_agents.training.config import TrainingConfig\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    " )\n",
    "logger = logging.getLogger(\"demo_capabilities\")\n",
    "\n",
    "print(\"Imports and logging setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Custom Reward Function: Politeness and Conciseness\n",
    "\n",
    "Here, we define a custom reward function that encourages two specific behaviors:\n",
    "-   **Politeness**: Rewards the agent for using polite phrases like \"thank you\" or \"I appreciate.\"\n",
    "-   **Conciseness**: Rewards for shorter responses (under 20 words) and penalizes overly long ones (over 50 words).\n",
    "\n",
    "This function is crucial for \"aligning\" the LLM's behavior with our desired objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolitenessReward:\n",
    "    \"\"\"\n",
    "    A custom reward function that encourages politeness and conciseness.\n",
    "    \"\"\"\n",
    "    async def compute_reward(self, turns: List[Any], scenario: Dict[str, Any]) -> Any:\n",
    "        # We only care about the assistant's last response\n",
    "        last_turn = turns[-1]\n",
    "        content = getattr(last_turn, \"content\", last_turn.get(\"content\", \"\"))\n",
    "            \n",
    "        score = 0.0\n",
    "        breakdown = {}\n",
    "\n",
    "        # Objective 1: Politeness (Keyword matching)\n",
    "        if re.search(r\"(thank you|appreciate|please)\", content, re.IGNORECASE):\n",
    "            score += 1.0\n",
    "            breakdown[\"politeness\"] = 1.0\n",
    "        else:\n",
    "            breakdown[\"politeness\"] = 0.0\n",
    "\n",
    "        # Objective 2: Conciseness (Length penalty)\n",
    "        word_count = len(content.split())\n",
    "        if word_count < 20:\n",
    "            score += 0.5\n",
    "            breakdown[\"conciseness\"] = 0.5\n",
    "        elif word_count > 50:\n",
    "            score -= 0.5\n",
    "            breakdown[\"conciseness\"] = -0.5\n",
    "        else:\n",
    "            breakdown[\"conciseness\"] = 0.0\n",
    "            \n",
    "        return {\"score\": score, \"breakdown\": breakdown}\n",
    "\n",
    "print(\"PolitenessReward class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Configuration\n",
    "\n",
    "We configure a `MultiTurnAgent` using a small pre-trained model (`Qwen/Qwen1.5-0.5B-Chat`) for quick demonstration purposes. In a real-world scenario, you would typically use a larger, more capable model (e.g., Llama-2-7b-chat-hf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen1.5-0.5B-Chat\" \n",
    "logger.info(f\"Configuring Agent with model: {model_name}\")\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    model_name=model_name,\n",
    "    system_prompt=\"You are a helpful and polite assistant.\",\n",
    "    max_new_tokens=32,  # Short generation for speed\n",
    "    temperature=0.8,\n",
    "    attn_implementation=\"eager\", # Explicitly use eager attention for compatibility\n",
    ")\n",
    "\n",
    "agent = MultiTurnAgent(agent_config)\n",
    "print(\"Agent configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Environment Setup\n",
    "\n",
    "A `ConversationEnvironment` simulates interactions between the user and the agent. We define simple scenarios for the agent to practice on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Setting up Training Environment\")\n",
    "scenarios = [\n",
    "    {\"id\": \"1\", \"context\": \"User complains.\", \"user_responses\": [\"Where is my package?\"]},\n",
    "    {\"id\": \"2\", \"context\": \"User greets.\", \"user_responses\": [\"Hi there!\"]}\n",
    "]
",
    "\n",
    "environment = ConversationEnvironment(\n",
    "    scenarios=scenarios,\n",
    "    max_turns=3,  # Short conversations for demo\n",
    "    persona=\"You are a customer service bot.\"\n",
    ")\n",
    "print(\"Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trainer Configuration (GRPO) and Training Loop\n",
    "\n",
    "Here, we set up the `MultiTurnGRPOTrainer` with our agent, environment, and custom reward function. We use a very short training run (`num_episodes=5`) for this demo.\n",
    "\n",
    "Crucially, the GRPO trainer will implicitly leverage the Rust-accelerated `stateset-rl-core` for efficient advantage computations during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Configuring GRPO Trainer\")\n",
    "training_config = TrainingConfig(\n",
    "    run_name=\"demo_politeness_agent\",\n",
    "    output_dir=\"./outputs/demo_agent\",\n",
    "    num_episodes=5,    # Very short run for demo purposes\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_generations=4, # Sample K=4 trajectories per input (GRPO key feature)\n",
    "    beta=0.01,         # KL penalty to keep it close to reference model\n",
    "    use_cpu=True,      # Force CPU for broad compatibility in this demo\n",
    "    report_to=\"none\",  # Disable W&B for this local demo\n",
    ")\n",
    "\n",
    "trainer = MultiTurnGRPOTrainer(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    reward_fn=PolitenessReward(),\n",
    "    config=training_config\n",
    ")\n",
    "\n",
    "logger.info(\"Starting Training Loop...\")\n",
    "logger.info(\"   (This implicitly uses stateset-rl-core Rust backend for advantage calculation)\")\n",
    "\n",
    "async def run_training_and_inference():\n",
    "    try:\n",
    "        await trainer.initialize()\n",
    "        \n",
    "        # FIX: Some models (like GPT-2) don't have a default chat template, so we define a simple one for the demo.\n",
    "        # Qwen models usually have a chat template, but this ensures compatibility for any model without one.",
    "        if hasattr(agent, \"tokenizer\") and agent.tokenizer.chat_template is None:\n",
    "             agent.tokenizer.chat_template = \"{% for message in messages %}{{ message['role'] + ': ' + message['content'] + '\n' }}{% endfor %}\"\n",
    "\n",
    "        trained_agent = await trainer.train()\n",
    "        logger.info(\"âœ… Training Complete!\")\n",
    "        \n",
    "        logger.info(\"\n--- Testing Trained Agent ---\")\n",
    "        test_history = [{\"role\": \"user\", \"content\": \"Where is my order?\"}]\n",
    "        response = await trained_agent.generate_response(test_history)\n",
    "        \n",
    "        logger.info(f\"User: Where is my order?\")\n",
    "        logger.info(f\"Agent: {response}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Demo failed: {e}\")\n",
    "        logger.info(\"Note: This demo requires PyTorch and Transformers to be installed.\")\n",
    "\n",
    "# Run the async function\n",
    "await run_training_and_inference()\n",
    "print(\"Training and inference complete. Check logs above for output.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
